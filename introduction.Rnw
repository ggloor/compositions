\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent

%set the bibliography style
\usepackage[super,sort&compress,comma]{natbib}
%\bibliographystyle{natbib}
\bibliographystyle{unsrt}
\renewcommand\bibnumfmt[1]{\emph{\textbf{\small{#1)}}}}
\setlength\bibsep{2pt}

% \VignetteIndexEntry{An R Package for determining differential abundance in high throughput sequencing experiments}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{wrapfig}
\usepackage[margin=1in,font=small,labelfont=bf,
               labelsep=colon]{caption}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}



\title{Why many high throughput sequencing experiments are irreproducible: an example from 16S rRNA gene sequencing.}
\author{Greg Gloor, ggloor@uwo.ca\\CSM Workshop: 2015}
\date{\today}                                           % Activate to display a given date or no date

\begin{document}

\maketitle
\tableofcontents
%\listoffigures
%\listoftables
\section{Abstract}
Fundamentally, many high throughput sequencing approaches generate similar data: samples contain many different features, the count  of reads-per-feature is tabulated for each sample, these counts are normalized, and finally the samples in each group are compared in some way.  The standard statistical tools used to analyze RNA-seq, ChIP-seq, 16S rRNA gene sequencing, metagenomics, etc. are fundamentally different for each approach  despite the underlying similarity in the data because the methods were developed in isolation for each experimental design, and often do not take into account the multivariate nature of the data. Here we show, using an example from 16S rRNA gene sequencing, that the approaches taken towards the analysis of these datasets suffers from several common pitfalls and that  widely used tools that treat the data as point estimates of the counts can lead to very misleading inferences.

\section{Introduction}
The human microbiome project has initiated the large-scale culture-independent analysis of microbial communities. However, many studies fail to replicate earlier studies even when the same technologies and strategies are used. For example, a multitude of studies have examined the link between autism and the human gut microbiota. These have variously implicated x,y,...,and z microbe as being linked to the condition. In a recent high-profile paper\cite{Hsiao:2013}, \emph{Bacillus fragilis} was suggested to restore some taxonomic groups to the gut microbiome of a mouse autism model. However, examination of the dataset shows that the conclusion was likely due to chance alone. While the autism dataset serves as a facile example, the literature are replete with other examples. 

All 16S rRNA gene sequencing datasets share a common origin. A microbial population is sampled, and DNA is isolated. One or more rRNA gene variable regions are PCR amplified using primers specific for the flanking constant regions. An aliquot of the resulting mixture of DNA fragments are then on a particular platform, generating hundreds of thousands to hundreds of millions of sequences that represent random samples of the PCR-amplified mixture. 

It is important to remember that the number of sequences obtained after sequencing contains no information about the \emph{number} of sequences in the PCR amplified pool, nor does it contain information about the \emph{number}  of molecules in the original DNA sample that was amplified. Instead the only information available is the \emph{relative} proportion of individual sequences in the PCR amplified mixture, which is assumed to approximate the proportion of sequences in the input DNA sample. Furthermore, the number of sequences obtained for a particular sample is determined entirely by the capacity of the sequencing platform. Thus, the outputs of the sequencing event are reads per operational taxonomic unit (OTU) per sample. The values across samples are often normalized by subsampling (rarefaction) or by converting to proportions or percentages; these latter values are widely spoken of in the literature as `relative abundances'. Subsampling is frequently used to estimate the associated sampling error. Some groups have begun advocating the use of  normalization methods prevalent in the RNA-seq field\cite{McMurdie:2014a}, but still treat the data as point estimates of the true abundance. There are three main data analysis issues that must be acknowledged. 

First, the nature of these data are misunderstood. As outlined above, the number of counts observed  per OTU are determined entirely by the capacity of the instrument and provide no information about the number of molecules in the input sample. Recall that both bacterial growth, and PCR are doubling processes and not linear processes, and so would be better modelled as log$_2$ differences.

Understanding that we are dealing with fold-change data is an explicit acknowledgement that the data do not map to normal Euclidian space where differences are linear.  Commonly used statistical tests expect linear differences between values and so are compromised to some degree, often catastrophically\cite{Aitchison:1986,vandenBoogaart2008320}. Therefore, the often-used approaches of converting the OTU count values to proportions or percentages and conducting statistical tests on those values, or of using data reduction strategies such as Principle Component Analysis on the  values are inappropriate because the differences between values are not linear. An alternative approach is to convert the OTU counts to ratios\cite{Aitchison:1986,aitchison:2005,Pawlowsky-Glahn:2006,pawlowsky2011compositional} which makes the differences between values linear, and so allows the use of common statistical tests. 
%However the user must interpret the output as ratios between OTU abundances rather than absolute differences\cite{pawlowsky2011compositional}. This approach is described below.

Second, high throughput sequencing (HTS) data represent samples of an unknown underlying large number of molecules. Thus, there is a large and unappreciated  error of estimation that is problematic when dealing with these data\cite{fernandes:2013}. The high error of estimation often results in false positive identification of differences, in fact, the statistical result can often be explained entirely by sampling variation.  This error is not captured by rarefaction or even acknowledged by other normalization methods and should be estimated and accounted for when deciding what is a significant difference. 

Third, 16S rRNA gene sequencing surveys, and similar experiments, contain many variables in each sample. Thus, any analysis that attempts to characterize the individual differences between groups must correct for the many hypotheses that are being tested. This step is often ignored, even in work published in very high profile journals subject to rigorous peer review. 

The purpose of these notes is to show  why HTS data for 16S rRNA gene sequencing, and any similar experiments such as RNA-seq, should be treated as ratio data and that it is possible to do so simply. We show that this approach accurately recapitulates the shape of the data for both constrained and unconstrained datasets. We show that converting the data to ratios can accurately model the very high variability at the low count margins, and that rarefaction under-estimates this variability. We use an example from the literature to show how ignoring these factors leads to improper conclusions.

\section{Results and Discussion}
\subsection{Sequencing technologies  randomly sample a pool of input DNA.} 

When generating a high throughput sequencing datseta it is important to understand the source of the data. In the case of 16S rRNA gene sequencing surveys, DNA is isolated and subsequently amplified using primers specific for constant regions that flank one or more 16S rRNA gene variable regions. A portion of the amplified DNA is then taken and used to generate a library, and only a portion of the library is sequenced. This workflow can be thought of as three random samples from an urn containing a random assortment of molecules. The first random sample is the sample from the environment itself that is used to make the DNA: what is swabbed, and how much is collected relative to the environment. Depending on the environment, this can be a large and complete sample or a small, incomplete and unrepresentative sample. The second random sample is the DNA molecules that are input into the initial PCR reaction. Here, the number of molecules available depends on the initial concentration of the DNA and the mean genome size of the organisms composing the sample. The third random sample comprise the DNA molecules from the library that are actually sequenced on the machine. 

As an example, if the mean genome size is 4 Mb, then 1 ng of DNA would provide approximately $1\times 10^6$ amplification templates if we assume 3-4 rRNA loci per genome. These templates would be amplified up to $2^{25}$ fold by PCR amplification, but only between $1 \times 10^6$ and $2 \times 10^8$ sequences are generated on the machine. These are apportioned across dozens or hundreds of samples giving typically 10000 - 100000 reads per sample. 

In the case of RNA-seq, the first random sample is the sample from the environment, the second is the mRNA placed into the cDNA reaction, the third random sample is the fraction of the cDNA that is used for the library.

\begin{figure}
\begin{center}

<<p_val, echo=FALSE, fig.height=5, fig.width=5>>=
# generate an empty matrix containing n rows, and 10,00 columns
n <- 1000
var <- 1000
x <- matrix(data=NA, nrow=n, ncol=var)
p <- matrix(data=NA, nrow=10, ncol=var) 
for(j in 1:10){
	# fill it with independent random (pseudo) numbers
	for(i in 1:n){ x[i,] <- sample(1:1000,var) }
	
	# un-comment this to show what happens when one variable is truly different
	# also try BH, bonferroni
	#x[1:(n/2),1:10] <-  x[1:(n/2),1:10] * 2

	# calculate p values
	p[j,] <- apply(x, 2, function(z){as.numeric(t.test(z[1:(n/2)], z[(n/2 + 1):n])[3])})
}	
mean.p <- apply(p, 2, mean)
p.adj <- apply(p, 1, function(n){p.adjust(n, method="BH")})
mean.p.adj <- apply(p.adj, 1, mean)
# plot a histogram


par(mfrow=c(2,2))
 hist(p[1,], breaks=1000, main="raw p", xlab="p")
 abline(v=0.05, col="red")
 hist(p.adj, breaks=1000, xlim=c(0,1), main="BH adj")
 abline(v=0.1, col="red")
 hist(mean.p, breaks=1000, xlim=c(0,1), main="mean raw p")
 abline(v=0.1, col="red")
hist(mean.p.adj, breaks=1000, xlim=c(0,1), main="mean BH adj")
  abline(v=0.1, col="red")
# count the number that have a p value less than 0.05
# length(which(p < 0.05))
# length(which(p.adj < 0.1))

@
\caption{P values represent the "probability of finding the observed sample results, or "more extreme" results, when the null hypothesis is actually true". In other words, a p-value is the probability of seeing a difference between groups when that difference is actually due to chance. The upper left panel in the figure shows the distribution of p values for a simulated experiment where there are 500 samples in group A and 500 samples in group B and 1000 variables in each sample. The upper right panel shows the Benjamini-Hochberg corrected p-values for the same experiment. The bottom left panel shows the mean p and mean BH corrected value of 10 replicates of the upper panels.  }
\label{pval}
\end{center}
\end{figure}

\subsection{Data are multivariate and, as a minimum, must be corrected for multiple hypothesis tests.}~ A p value is the likelihood of observing the result, or one more extreme, by chance alone. The commonly used cutoff of 0.05 means that we are almost certain to observe many false positive results when the samples contain many variables. Figure \ref{pval} shows an example. Here, I have chosen 1000 random numbers for each of 1000 samples. Then, I split the samples, arbitrarily, into two groups. Finally, I conducted an unpaired Welch's t-test on each of the 1000 variables in the two groups of 500 samples. The upper left panel shows a histogram of the results: p$<= 0.05$ for approximately 50 of the 1000 variables. These are shown as the p values to the left of the red line. You should be very wary of anyone who shows multivariate data, and then makes conclusions from raw p values. 

One commonly used approach is to correct your p values for multiple hypothesis tests using one of the many corrections. The Benjamin-Hochberg procedure is widely used, and corrects the raw p values such that the likelihood of observing a given corrected value is the adjusted value. This is called a False Discovery Rate correction. For example, if the p value is 0.001, and the BH corrected p value is 0.1, then the likelihood that the difference is observed by chance is 10\%. Thus, if you have 100 variables that have a BH value less than 0.1, you expect that 10 of them will be false positives - you just don't know which ones. The alternative is a Family Wide Error Rate correction, the most famous of which is the Bonferroni correction. In this case the value reported is the likelihood that any of the values reported are wrong. So if 100 values are reported using a FWER cutoff of 0.1, then the likelihood that any of them are wrong is 10\%.

Another approach is to determine if the p values are stable to sampling variation. This approach is shown in the lower two panels of Figure \ref{pval}. The bottom left shows the mean p values for 10 random replicates of the data in the top left panel. Here we see that the mean p value is approximately 0.5 because the expected p value for a randomly chosen comparison is 0.5. Note that the mean BH adjusted p values approach 1.

%As an example if the mean genome size is 4 Mb, a mole of genomes would have a mass of $2.64 \times 10^9$ grams. If the DNA concentration after isolation is 1 ng$/ \mu$l, and one $\mu$l of DNA is used for the amplification, this corresponds to ($1\times 10^{-9} \mathrm{\ g}) / (2.64 \times 10^9\ \mathrm{g/mole})  \times (6.02 \times 10^{23} \mathrm{\ genomes/mole}) = 228,000$ genomes. Each genome further has 3-4 rRNA gene loci, providing approximately $1\times 10^6$ amplification templates. Thus, if there are 10 $\mu$l of DNA isolated at 1 ng/$\mu$l, we are sampling 10\% of the input and different samples will give us compositions that differ slightly. Depending on the platform between $1 \times 10^6$ and $2 \times 10^8$ sequences are generated, but these are apportioned across dozens or hundreds of samples giving typically 10000 - 100000 reads per sample.  

%These rRNA genes in these genomes are exponentially amplified by the PCR, to a typical concentration of $> 10^{11}$ molecules per $\mu$l (assuming a length of 250 bp, and a final concentration of approximately 100 ng/ $\mu$l). 

\subsection{Sequencing can change the shape of the data:}

\begin{figure}
\begin{center}

<<constrained_data,echo=FALSE,fig.width=6.5,fig.height=5>>=

#RAW DATA ON MBQC DISK
#Original script at: "/Users/ggloor/Documents/0_techrep_blog/analysis_joint


rdirichlet <- function (n, alpha)
{
  if(length(n) > 1) n <- length(n)
  #if(length(n) == 0 || as.integer(n) == 0) return(numeric(0))
  #n <- as.integer(n)
  if(n < 0) stop("value(n) can not be negative in rtriang")
  
  if(is.vector(alpha)) alpha <- t(alpha)
  l <- dim(alpha)[2]
  x <- matrix(rgamma(l * n, t(alpha)), ncol = l, byrow=TRUE)  # Gere le recycling
  return(x / rowSums(x))
}

num.one = 100 # the number of rare-counts in the dataset

mat.double <- matrix(data=NA, nrow=20, ncol=num.one + 10)
prop.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
clr.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)

mat.double.u <- matrix(data=NA, nrow=20, ncol=num.one + 10)
prop.mat.u <- matrix(data=NA, nrow=20, ncol=num.one + 10)
clr.mat.u <- matrix(data=NA, nrow=20, ncol=num.one + 10)

# constant sum input
minimum.count <- 1 # multiplier to set minimum count for in.put
# non-constant sum input with both one big increase
in.put <- c(10,20971,1,1,5,10,20,50,100,200,1000) * minimum.count

total.sum <- sum(in.put + 1) * 1000

for(i in 0:19){
	# constant sum input
	junk <- in.put * c(2^i, rep(1,num.one + 9))
	junk[3] <- total.sum - sum(junk)
	mat.double[i+1,] <- junk
	prop.mat[i+1,] <- as.numeric( rdirichlet(1, junk) )
	clr.mat[i+1,] <- log2(prop.mat[i+1,]) - mean(log2(prop.mat[i+1,]))
}

for(i in 0:19){
	# non-constant sum input
	#junk <- in.put * c(2^i, rep(1,num.one + 9))
	junk <- in.put * c(2^i, rep(1,num.one + 9))
	mat.double.u[i+1,] <- junk
	prop.mat.u[i+1,] <- as.numeric( rdirichlet(1, junk) )
	clr.mat.u[i+1,] <- 2^(log2(prop.mat.u[i+1,]) - mean(log2(prop.mat.u[i+1,])))
}

par(mfrow=c(2,4), mar=c(4,4,3,1) )

plot(mat.double[,1], pch=20, type="b",  ylim=c(min(mat.double), max(mat.double)), xlab="time point", ylab="raw count")
title( main="Constrained\ninput", adj=0.5)
points(mat.double[,2], type="b",pch=21, col="gray")
points(mat.double[,3], type="b",pch=22, col="orange")
points(mat.double[,num.one + 10], type="b",pch=23, col="blue")
points(mat.double[,num.one+4], type="b",pch=24, col="red")

plot(prop.mat[,1], pch=20, type="b", ylim=c(min(prop.mat[,num.one+4]), max(prop.mat)), xlab="time point", ylab="raw proportion")
title( main="Constrained\nproportion", adj=0.5)
points(prop.mat[,2], type="b", pch=21, col="gray")
points(prop.mat[,3], type="b", pch=22, col="orange")
points(prop.mat[,num.one+10], type="b", pch=23, col="blue")
points(prop.mat[,num.one+4], type="b", pch=24, col="red")

plot(mat.double[,1], pch=20, type="b", log="y", ylim=c(min(mat.double), max(mat.double)), xlab="time point", ylab="log10 count")
title( main="Constrained\ninput", adj=0.5)
points(mat.double[,2], type="b",pch=21, col="gray")
points(mat.double[,3], type="b",pch=22, col="orange")
points(mat.double[,num.one + 10], type="b",pch=23, col="blue")
points(mat.double[,num.one+4], type="b",pch=24, col="red")

plot(prop.mat[,1], pch=20, type="b", ylim=c(min(prop.mat[,num.one+4]), max(prop.mat)), xlab="time point", log="y", ylab="log10 proportion")
title( main="Constrained\nproportion", adj=0.5)
points(prop.mat[,2], type="b", pch=21, col="gray")
points(prop.mat[,3], type="b", pch=22, col="orange")
points(prop.mat[,num.one+10], type="b", pch=23, col="blue")
points(clr.mat[,num.one+4], type="b", pch=24, col="red")

# unconstrained
plot(mat.double.u[,1], pch=20, type="b",  ylim=c(min(mat.double.u), max(mat.double.u)), xlab="time point", ylab="raw count")
title( main="Unconstrained\ninput", adj=0.5)
points(mat.double.u[,2], type="b",pch=21, col="gray")
points(mat.double.u[,num.one + 10], type="b",pch=23, col="blue")
points(mat.double.u[,num.one+4], type="b",pch=24, col="red")

plot(prop.mat.u[,1], pch=20, type="b", ylim=c(min(prop.mat.u[,num.one+4]), max(prop.mat.u)), xlab="time point", ylab="raw proportion")
title( main="Unconstrained\nproportion", adj=0.5)
points(prop.mat.u[,2], type="b", pch=21, col="gray")
points(prop.mat.u[,num.one+10], type="b", pch=23, col="blue")
points(prop.mat.u[,num.one+4], type="b", pch=24, col="red")

plot(mat.double.u[,1], pch=20, type="b", log="y", ylim=c(min(mat.double.u), max(mat.double.u)), xlab="time point", ylab="log10 count")
title( main="Unconstrained\ninput", adj=0.5)
points(mat.double.u[,2], type="b",pch=21, col="gray")
points(mat.double.u[,num.one + 10], type="b",pch=23, col="blue")
points(mat.double.u[,num.one+4], type="b",pch=24, col="red")


plot(prop.mat.u[,1], pch=20, type="b", ylim=c(min(prop.mat.u[,num.one+4]), max(prop.mat.u)),log="y", xlab="time point", ylab="log10 proportion")
title( main="Unconstrained\nproportion", adj=0.5)
points(prop.mat.u[,2], type="b", pch=21, col="gray")
points(prop.mat.u[,num.one+10], type="b", pch=23, col="blue")
points(prop.mat.u[,num.one+4], type="b", pch=24, col="red")

@
\caption{High-throughput sequencing affects the shape of the data differently on constrained and unconstrained data. The two left panels show the absolute number of reads in the input tube for 20 steps where the green and black OTUs are changing abundance by 2-fold each step. The gray, blue and red OTUs are held at a constant number in each step in both cases.  The second column shows the output in proportions (or ppm, or FPKM) after random sampling to a constant sum, as occurs on the sequencer. The orange OTU in the constrained data set is much more abundant than any other, and is changing to maintain a constant number of input molecules.  Samples in the two right columns are the same values plotted on a log scale on the Y-axis for convenience. Note how the constrained data is the same before and after sequencing while the unconstrained data is severely distorted. }
\label{constant}
\end{center}
\end{figure}
It is  assumed that the output from a high-throughput sequencing experiment represents in some way the underlying abundance of the input DNA molecules. The input counts panels on the left side of Figure \ref{constant} shows two  idealized experiments. The top left shows the case where the total count of all nucleic acid species in the input is constrained, the bottom left illustrates the case where the total count is unconstrained. These are modelled as a time series, but any  process would produce the same results. 

Constrained datasets  occur if the increase or decrease in any component is exactly compensated by the increase or decrease of one or more others. Here the total  count  remains constant across all experimental conditions. Examples of constrained datasets would include  allele frequencies at a locus where the total has to be 1, and the RNA-seq where the induction of genes occurs in a steady-state cell culture. In this case, any process, such as sequencing that generates a proportion simply recapitulates the data with sampling error. The unspoken assumption in most high throughput experimental designs is that this assumption is true---\emph{ it is not!}

An unconstrained dataset  results if the total count is free to vary. Examples of unconstrained datasets would include ChIP-Seq, RNA-seq where we are examining two  different conditions or cell populations, metagenomics, etc. Importantly, 16S rRNA gene sequencing analyses are almost always free to vary; that is, the total bacterial load is rarely constant in an environment. Thus, the unconstrained data type would be the predominant type of data that would be expected.

The relative abundance panels on the right side of Figure \ref{constant} shows the result of random sampling with a defined maximum value in these two types of datasets. This random sampling reflects the data that results from high throughput sequencing where the total number of reads is constrained by the instrument capacity. The data is represented as a proportion, but  scales to parts per million or parts per billion  without changing the shape. Here we see that the shape of the data after sequencing is very similar to the input data in the case of constrained, but is very different in the case of non-constrained data. In the unconstrained dataset, observe how the blue and red features appear to be constant over the first 10 time points, but then appear to decrease in abundance at later time points. Conversely, the black feature appears to increase linearly at early time points, but appears to become constant at  late time points. Obviously, we would misinterpret what is happening if we compared early and late timepoints in the unconstrained dataset. It is also worth noting how the act of random sampling makes the proportional abundance of the rare OTU species uncertain in both the constrained and unconstrained data, but has little effect on the relative apparent effect on the relative abundance of OTUs with high counts.

%We assume that the abundance of each input DNA species that is observed after sequencing reflects a random sample of the input  molecules. We can see that  that this may indeed be the case if the total number of  molecules in the input sample is constant. This constraint would be met if, for example, an increase in one or more DNA species was balanced with an equivalent decrease in one or more different species. Such a constraint would be met In the figure, the red and blue OTU sequences are held constant in each sample, the green OTU is decreasing by 2 fold and the black OTU is increasing by 2 fold in each subsequent sample. The abundance of the orange OTU is adjusted such that the total sum of the OTU sequences in the input is held constant. Here it can be seen that the input counts and the relative abundance of each species following sequence have similar shapes, with the exception that the rarest species display significant variability because of random sampling.


%%%%%%%%%%%%%%%%
%\newpage
\subsection{Commonly used transformations are misleading}
Current practice is to examine the datasets using `relative abundance' values, that is, the proportional abundance of the OTUs either before or after normalization for read depth. This approach is equivalent to examining the input unconstrained data of the type seen in Figure \ref{constant} in the relative abundance sample space in the bottom right panel of the figure. This approach will obviously lead to incorrect assumptions in at least some cases. For example, depending upon the steps chosen to compare, the blue OTU, that has constant counts in the input, will be seen to either increase or decrease in abundance. Conversely, the green OTU, that is always decreasing in abundance will be seen to be constant if comparing samples 1-8. 

The ecological literature offers many different transformations for such data, often as a way of making the data appear `more normal'. Figure  \ref{transform} shows the results of five such transformations. \begin{itemize}
\item The frequency transform divides the each OTU value by the largest OTU count, and then divides the resulting values by the number of OTUs in the sample that had non-zero counts. 
\item The Hellinger transformation that takes the square root of the relative abundance (proportion) value. 
\item The range transform standardizes the values to have a range from 0 to 1. OTUs with 0 counts are set to 0. 
\item The standardize transform standardizes the values for each sample to have a mean of 0 and a variance of 1. I
\item The log transform divides each OTU count in a sample by the minimum non-zero count value, then takes the logarithm of the resulting value and adds 1. Counts of 0 are assigned a value of 0 to avoid taking the logarithm of 0. 

\item The centred log-ratio transformation divides the OTU values by the geometric mean OTU abundance, and then takes the logarithm. 
\end{itemize}

It is obvious that the first four transformations result in data that badly mis-represents the shape of the actual input data. The log transformation, however results in the shape of the output data approximating the shape of the input data, except that the uncertainty of each data point is large. The ratio transform his transformation accurately recapitulates the shape of the original input data, and more accurately represents the uncertainty of each data point. 


\begin{figure}
\begin{center}

<<transformations,echo=FALSE,fig.width=6,fig.height=4,warning=F,message=F>>=

#RAW DATA ON MBQC DISK
#Original script at: "/Users/ggloor/Documents/0_techrep_blog/analysis_joint

library(vegan)
rdirichlet <- function (n, alpha)
{
  if(length(n) > 1) n <- length(n)
  #if(length(n) == 0 || as.integer(n) == 0) return(numeric(0))
  #n <- as.integer(n)
  if(n < 0) stop("value(n) can not be negative in rtriang")
  
  if(is.vector(alpha)) alpha <- t(alpha)
  l <- dim(alpha)[2]
  x <- matrix(rgamma(l * n, t(alpha)), ncol = l, byrow=TRUE)  # Gere le recycling
  return(x / rowSums(x))
}

num.one = 100

mat.double <- matrix(data=NA, nrow=20, ncol=num.one + 10)
prop.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
clr.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
hel.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
log.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
freq.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
range.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)
pa.mat <- matrix(data=NA, nrow=20, ncol=num.one + 10)

# non-constant sum input
minimum.count <- 1 # multiplier to set minimum count for in.put
# non-constant sum input with both one big increase and one big decrease
in.put <- c(10,20971,1,1,5,10,20,50,100,200,1000) * minimum.count
total.sum <- sum(in.put + 1) * 1000

for(i in 0:19){
	# non-constant sum input
	#junk <- in.put * c(2^i, rep(1,num.one + 9))
	junk <- in.put * c(2^i, rep(1 ^ i,num.one + 9))
	mat.double[i+1,] <- junk
	prop.mat[i+1,] <- as.numeric( rdirichlet(1, junk) )
	clr.mat[i+1,] <- 2^(log2(prop.mat[i+1,]) - mean(log2(prop.mat[i+1,])))
	hel.mat[i+1,] <- sqrt(prop.mat[i+1,])
	freq.mat[i+1,] <- decostand(prop.mat[i+1,], method="freq")
	log.mat[i+1,] <-  2^(decostand(prop.mat[i+1,], method="log")) #this is the divide by the minimum transformation from decostand that precedes taking the logarithm
	range.mat[i+1,] <- decostand(prop.mat[i+1,], method="range")
	pa.mat[i+1,] <- 10^decostand(prop.mat[i+1,], method="standardize")
}

par(mfrow=c(2,3), mar=c(4,4,3,1))

plot(freq.mat[,1], pch=20, type="b", log="y", ylim=c(min(freq.mat[,num.one+4]), max(freq.mat)),main="frequency", xlab="time point", ylab="value")
#points(freq.mat[,2], type="b", pch=19, col="green")
#points(freq.mat[,3], type="b", pch=19, col="orange")
points(freq.mat[,num.one+10], type="b", pch=23, col="blue")
points(freq.mat[,num.one+4], type="b", pch=24, col="red")

plot(hel.mat[,1], pch=20, type="b", log="y", ylim=c(min(hel.mat[,num.one+4]), max(hel.mat)),main="hellinger", xlab="time point", ylab="value")
#points(hel.mat[,2], type="b", pch=19, col="green")
#points(hel.mat[,3], type="b", pch=19, col="orange")
points(hel.mat[,num.one+10], type="b", pch=23, col="blue")
points(hel.mat[,num.one+4], type="b", pch=24, col="red")

plot(range.mat[,1], pch=20, type="b", log="y", ylim=c(min(range.mat[,num.one+4]), max(range.mat)),main="range ", xlab="time point", ylab="value")
#points(range.mat[,2], type="b", pch=19, col="green")
#points(range.mat[,3], type="b", pch=19, col="orange")
points(range.mat[,num.one+10], type="b", pch=23, col="blue")
points(range.mat[,num.one+4], type="b", pch=24, col="red")

plot(pa.mat[,1], pch=20, type="b", log="y", ylim=c(min(pa.mat[,num.one+4]), max(pa.mat)),main="standardize", xlab="time point", ylab=" value")
#points(pa.mat[,2], type="b", pch=19, col="green")
#points(pa.mat[,3], type="b", pch=19, col="orange")
points(pa.mat[,num.one+10], type="b", pch=23, col="blue")
points(pa.mat[,num.one+4], type="b", pch=24, col="red")

plot(log2(log.mat[,1]), pch=20, type="b", log="y", ylim=c(log2(min(log.mat[,num.one+4])), log2(max(log.mat))),main="log", xlab="time point", ylab="value")
#points(log.mat[,2], type="b", pch=19, col="green")
#points(log.mat[,3], type="b", pch=19, col="orange")
points(log2(log.mat[,num.one+10]), type="b", pch=23, col="blue")
points(log2(log.mat[,num.one+4]), type="b", pch=24, col="red")

plot(log2(clr.mat[,1]), pch=20, type="b",  ylim=c(log2(min(clr.mat[,num.one+4])), log2(max(clr.mat))),main="clr", xlab="time point", ylab=" value")
#points(clr.mat[,2], type="b", pch=19, col="green")
#points(clr.mat[,3], type="b", pch=19, col="orange")
points(log2(clr.mat[,num.one+10]), type="b", pch=19, col="blue")
points(log2(clr.mat[,num.one+4]), type="b", pch=19, col="red")
@
\caption{The effect of ecological transformations on unconstrained high throughput sequencing datasets. Data generated as in Figure \ref{constant} were transformed with five different approaches implemented in the vegan ecological analysis package, and with the entered log-ratio approach suggested by Aitchison.   }
\label{transform}
\end{center}
\end{figure}




\subsection{A count of 0 does not mean that you expect 0!}
A common misconception to normalizing by the geometric mean is that the geometric mean is not defined if any of the values in a sample are 0. While this is true, values of 0 for an OTU can arise because the OTU sequence could not occur in the experiment, or because the OTU could exist in one group but not the other, or because the OTU was very rare in one or more samples making its selection from the library subject to chance. In the first case, the OTU would not be found in any sample, and that OTU could simply be deleted from the dataset without effect. In the second case, the OTU would be represented in one group but not the other. In the third case, where an OTU has at least one count in at least one sample,  a value of 0 could arise in other samples because of sequencing depth. In the latter two  cases, it is possible that the OTU could  have been detected if more reads per sample were obtained or if more replicates of the library were sequenced.  

Current practice in 16S rRNA gene sequencing studies is to assume that an observed value of 0 in a sample represents the actual value. In RNA-seq it is common to remove all genes where the total sum across all samples is small (usually with a mean of 2 or less and no more than about 10 counts in any sample). In either case, the assumption is that variables with very low counts are irrelevant. 

This assumption was  tested by sequencing the same library from 16 different samples on two individual Illumina runs, and then determining the OTU count in one run if the OTU had a count of 0, 1, 2, or 3 in the other run. Figure \ref{replicate} shows the result, and it can be seen that the count observed for an OTU in one replicate is often very different from the count observed for the other replicate. Similar observations hold for RNA-seq. It is clear that the absolute number of counts observed varies between replicates and as expected the underlying distributions approximate what would be expected for random sample of the input library. The uncertainty in ascertaining the true values for OTUs with low counts is the reason that the log transformation in Figure \ref{transform} injects undesired variability into the data. 
\begin{figure}
\begin{center}

<<zero_data,echo=FALSE,fig.width=4,fig.height=4>>=

#RAW DATA ON MBQC DISK
#Original script at: "/Users/ggloor/Documents/0_techrep_blog/analysis_joint

#original data
data <- read.table("/Users/ggloor/Documents/0_techrep_blog/analysis_exact/td_OTU_tag_mapped_exact.txt", comment.char="",sep="\t", header=T, check.names=F, skip=1, row.names=1)
newdR <- data[order(colnames(data))]

run.1 <- c(1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31)
run.2 <- run.1 + 1
if0 <- c(newdR[,run.1][newdR[,run.2] == 0], newdR[,run.2][newdR[,run.1] == 0])
if1 <- c(newdR[,run.1][newdR[,run.2] == 1], newdR[,run.2][newdR[,run.1] == 1] )
if2 <- c(newdR[,run.1][newdR[,run.2] == 2], newdR[,run.2][newdR[,run.1] == 2] )
if3 <- c(newdR[,run.1][newdR[,run.2] == 3], newdR[,run.2][newdR[,run.1] == 3])
par(mfrow=c(2,2), mar=c(3, 2, 2, 2))
barplot(table(if0), col=c("black", rep("gray", 6)))
title("OTU=0", adj=0, cex=0.7)
barplot(table(if1), col=c(rep("gray",1), "black", rep("gray", length(table(if1) -2))) )
title("OTU=1", adj=0, cex=0.7)
barplot(table(if2), col=c(rep("gray",2), "black", rep("gray", length(table(if2) -3))) )
title("OTU=2", adj=0, cex=0.7)
barplot(table(if3), col=c(rep("gray",3), "black", rep("gray", length(table(if3) -4))) )
title("OTU=3", adj=0, cex=0.7)
@
\caption{The distribution of counts in a replicate OTU when the first OTU has counts between 0 and 3. The same library of sixteen different 16S rRNA gene amplification samples were sequenced on two different Illumina HiSeq runs, and the count of OTUs that had values of 0 to 3 in one replicate, shown by the black bar, were tabulated for the other replicate, shown by the grey bar. Sequencing depth for each replicate was within 10\% of the the other replicate. }
\label{replicate}

\end{center}
\end{figure}

\begin{figure}
\begin{center}

<<rare_vs_Dir,echo=FALSE,fig.width=6,fig.height=3.7>>=

#RAW DATA ON MBQC DISK
#Original script at: "/Users/ggloor/Documents/0_techrep_blog/analysis_joint


rdirichlet <- function (n, alpha)
{
  if(length(n) > 1) n <- length(n)
  #if(length(n) == 0 || as.integer(n) == 0) return(numeric(0))
  #n <- as.integer(n)
  if(n < 0) stop("value(n) can not be negative in rtriang")
  
  if(is.vector(alpha)) alpha <- t(alpha)
  l <- dim(alpha)[2]
  x <- matrix(rgamma(l * n, t(alpha)), ncol = l, byrow=TRUE)  # Gere le recycling
  return(x / rowSums(x))
}

prior <- 0.5 #dirichlet and plotting prior

#original data
data <- read.table("/Users/ggloor/Documents/0_techrep_blog/analysis_exact/td_OTU_tag_mapped_exact.txt", comment.char="",sep="\t", header=T, check.names=F, skip=1, row.names=1)
newdR <- data[order(colnames(data))]

d.21 <- data.frame(newdR[,"CACTAC-CACTAC-1"], newdR[,"CACTAC-CACTAC-2"])

rownames(d.21) <- rownames(newdR)

colnames(d.21) <- c("Rep_A", "Rep_B")

d.21[d.21 == 0] <- prior

round.digits <- 1 # This affects the low end variation estimate strongly
# dirichlet
xa <- round(rdirichlet(100000, d.21[,"Rep_A"]) *  sum(d.21[,"Rep_A"]), round.digits)
xa[ xa==0 ] <- prior
ya <- apply(xa, 1, function(x){ abs(log2(x) - log2(d.21[,"Rep_A"]) ) })
ya.q <- apply(ya, 1, function(x) {quantile(x,probs=c(0.05,0.4,0.99))})
yaE <- apply(xa, 1, function(x){ log2(abs(x- d.21[,"Rep_A"]) ) })
yaE[ is.nan(yaE) ] <- 0
yaE.q <- apply(yaE, 1, function(x) {quantile(x,probs=c(0.05,0.4,0.99))})

# rarefaction dataset
#rare1 <- matrix(data=NA, nrow=1000,ncol=nrow(newdR))
#rare2 <- matrix(data=NA, nrow=1000,ncol=nrow(newdR))
#rare_R_1 <- matrix(data=NA, nrow=1000,ncol=nrow(newdR))
#rare_R_2 <- matrix(data=NA, nrow=1000,ncol=nrow(newdR))
#for(i in 0:999){
#	file = paste("/Users/ggloor/Documents/0_techrep_blog/analysis_exact/qiime/jknife_txt/jknife",i,".txt", sep="")
#	d <- read.table(file,comment.char="", header=T, check.names=F, skip=1, row.names=1, sep="\t")
#	j <- i+1
#	rare1[j,] <- d[,"CACTAC-CACTAC-1"]
#	rare2[j,] <- d[,"CACTAC-CACTAC-2"]
#	file = paste("/Users/ggloor/Documents/0_techrep_blog/analysis_exact/qiime/rare_txt/jknife",i,".txt", sep="")
#	d <- read.table(file,comment.char="", header=T, check.names=F, skip=1, row.names=1, sep="\t")
#	j <- i+1
#	rare_R_1[j,] <- d[,"CACTAC-CACTAC-1"]
#	rare_R_2[j,] <- d[,"CACTAC-CACTAC-2"]
#}
#write.table(rare_R_1, file="rare_R_1.txt")
#write.table(rare_R_2, file="rare_R_2.txt")

rare1 <- read.table("~/Documents/0_techrep_blog/analysis_exact/rare_R_1.txt")
rare2 <- read.table("~/Documents/0_techrep_blog/analysis_exact/rare_R_2.txt")

#correction for number of reads in rarefied samples
rare2[rare2 == 0] <- prior
rare1[rare1 == 0] <- prior
ra <- apply(rare1, 1, function(x){ abs(log2(x/(10000/sum(d.21[,"Rep_A"]))) - log2(d.21[,"Rep_A"]) ) })
ra.q <- apply(ra, 1, function(x) {quantile(x,probs=c(0.05,0.4,0.99))})

raE <- apply(rare1, 1, function(x){ log2(abs(x/(10000/sum(d.21[,"Rep_A"])) - d.21[,"Rep_A"]) ) })
raE[ is.nan(raE) ] <- 0
raE.q <- apply(raE, 1, function(x) {quantile(x,probs=c(0.05,0.4,0.99))})

# resampling
rare2[rare2 == 0] <- prior
raR <- apply(rare2, 1, function(x){ abs(log2(x/(10000/sum(d.21[,"Rep_A"]))) - log2(d.21[,"Rep_A"]) ) })
raR.q <- apply(raR, 1, function(x) {quantile(x,probs=c(0.05,0.4,0.99))})

raRE <- apply(rare2, 1, function(x){ log2(abs(x/(10000/sum(d.21[,"Rep_A"])) - d.21[,"Rep_A"]) ) })
raRE[ is.nan(raRE) ] <- 0
raRE.q <- apply(raRE, 1, function(x) {quantile(x,probs=c(0.05,0.4,0.99))})


#A estimated variance plotted vs A-B difference
mincount <- apply(d.21[,1:2], 1, min)
rat <- sum(d.21[,1]) / sum(d.21[,2]) 

par(mfrow=c(1,2))

plot( log2(mincount), abs(log2(abs(d.21[,"Rep_A"] - round( rat * d.21[,"Rep_B"], 1)))), xaxt="n", yaxt="n", main="Difference", pch=19, cex=1.1, col=rgb(0,0,0,0.3),  ylab="Absolute Difference", xlab="Feature Count", ylim=c(0,8), type="h", lwd=4)
points(loess.smooth(x=log2(d.21[,"Rep_A"]), y= raE.q[3,]), type="l", col="red", lwd=2)
points(loess.smooth(x=log2(d.21[,"Rep_A"]), y= raRE.q[3,]), type="l", col="orange", lwd=2)
points(loess.smooth(x=log2(d.21[,"Rep_A"]), y= yaE.q[3,]), type="l", col="blue", lwd=2)
axis(1, at=log2(c(1,10,100,1000,10000)), labels=c(1,10,100,"1e3","1e4"))
axis(2, at=log2(c(1,10,100)), labels=c(1,10,100))

#rect(log2(1e3), -1, log2(1e5), 10.5,  col=rgb(0,0,0,0.15), lwd=NULL, border = NA)
#rect(2, -1, 5, 10.5,  col=rgb(.64,.2,.06,0.15), lwd=NULL, border = NA)


plot( log2(mincount), abs(log2(d.21[,"Rep_A"]) - log2(round(rat * d.21[,"Rep_B"], 1))), xaxt="n", yaxt="n", main="Ratio ", pch=19, cex=1.1, col=rgb(0,0,0,0.3),  ylab="Replicate Ratio", xlab="Feature Count", ylim=c(0,3), type="h", lwd=4)
points(loess.smooth(x=log2(d.21[,"Rep_A"]), y= ra.q[3,]), type="l", col="red", lwd=2)#points(loess.smooth(x=log2(d.21[,"Rep_A"]), y= raR.q[3,]), type="l", col="orange", lwd=2)
#points(log2(d.21[,"Rep_A"]),raR.q[3,], pch=19,cex=0.5, col="orange")
points(loess.smooth(x=log2(d.21[,"Rep_A"]), y= ya.q[3,]), type="l", col="blue", lwd=2)
#points(log2(d.21[,"Rep_A"]), ya.q[3,], pch=19, cex=0.5, col=rgb(0,0,1,0.4))
axis(1, at=log2(c(1,10,100,1000,10000)), labels=c(1,10,100,"1e3","1e4"))
axis(2, at=log2(c(1,2,4,8)), labels=c(1,2,4,8))

#rect(-2, -1, 2, 5.5,  col=rgb(0,0,0,0.15), lwd=NULL, border = NA)
#rect(2, -1, 5, 5.5,  col=rgb(.64,.2,.06,0.15), lwd=NULL, border = NA)

@
\caption{Examining technical replicate variability of 16S rRNA gene sequencing experiments as linear differences and as ratios. The same input DNA library for fourteen samples was sequenced on two different HiSeq lanes to estimate the technical variability. The difference between the counts for each OTU in replicate A and B were plotted in two ways. First, as differences between counts in replicates A and B, and second, as the ratio between the counts observed in replicates A and B. The grey bars show the actual difference or ratio observed plotted vs. the minimum value observed for each OTU in the replicates. The estimated variability in the data was determined in three ways: 1), by rarefying the data with the Jacknife or 2) with the Bootstrap approach in QIIME, or 3) by drawing instances from the Dirichlet distribution. The 99th percentile of the difference between these random instances and the actual data found by these three approaches is shown as the red, orange or blue line.  }
\label{rarefy}
\end{center}
\end{figure}


\subsection{The uncertainty is relative}

That the true count of an OTU cannot be determined from a single sequencing run, suggests that we might be better to estimate the range of values that an OTU can assume, and to determine how these estimates change the results of the analysis performed.  case, we do not know the true underlying value for the OTU count and must estimate it. There are two approaches used to estimate the uncertainty of 16S rRNA gene sequencing experiments. 

Subsampling, or rarefaction, is performed to normalize all samples in 16S rRNA gene sequencing to a common sequencing depth, and to estimate the variability in the data for downstream procedures such as $\alpha$- and $\beta$-diversity analyses. An alternative to subsampling is the draw instances of the data from the Dirichlet distribution that treats each variable as a multinomial Poisson instance with the constraint that the values sum to 1. We have shown previously that the instances of the Dirichlet distribution accurately reflect the underlying technical variation in RNA-seq datasets (see Figure 1 in \cite{fernandes:2013}). Note that when sampling occurs from an Poisson process that the error expected is relatively large for low counts and relatively low for high counts because the expected value and the variance of the data are equal. Thus, the expected error is 100\% for a count of 1, and 10\% for a count of 100 and 1\% for a count of 10000, etc. 

One caveat with Dirichlet subsampling is that it is a Baysian approach to estimate the distribution of frequencies for each OTU. Thus, we must include our prior belief of the actual abundance of each OTU before sampling. This is often thought to inject investigator bias into the analysis. The bias will be most extreme at the margins of the estimation because a value close to 0 indicates a prior belief that the OTU should never have been detected, while a value close to 1 indicates a prior belief that the OTU should have been detected with certainty. Thus the least biased value in general should be 0.5\cite{Jaynes:2003}. In practice we observe that the choice of prior has little effect on the outcome provided it is not close to 0 or 1.

We examined how well both procedures modelled the actual underlying variation in OTU counts from the replicate samples used in Figure \ref{replicate}. Monte-Carlo instances of the data drawn from the Dirichlet distribution were generated and multiplied by the number of total reads for the sample. Subsampling was  performed with QIIME\cite{Caporaso:2010a} using the Jacknife procedure (i.e., subsampling from an original distribution without replacement\cite{Efron:1981}), and with the Bootstrapping procedure\cite{Efron:1981} (i.e., subsampling with replacement) using a sampling depth of 10000 using the multiple\_rarefactions.py script in QIIME v1.8.0. This reduced the read depth for the samples by a factor between 2 and 4.5 fold, which is well within the subsampling parameters of recent experiments. The difference between the read counts observed in the subsampled, Dirichret, and actual data was calculated as for the replicate difference. 


Figure \ref{rarefy} shows an example  plot of the technical variability that occurs at the OTU level for the same samples sequenced to approximately the same sequencing depth on two separate Illumina lanes. Zero counts, if they occurred in one replicate were replaced with 0.5\cite{fernandes:2013,martin:2003}. The actual technical variability is shown as the bars, and is plotted as the absolute difference between replicate 1 and replicate 2. The density of the bars illustrate the number of observations at that co-ordinate, with darker bars representing more observations. The red and orange lines in Figure \ref{rarefy} show the loess line of best fit for the 99$^{th}$ percentile of the difference between the actual and the Jackknifed and bootstrapped rarefied data, and the blue line shows the 99th percentile for the Dir instances. 

The left plot shows that when treating the data as absolute counts, the difference between the actual replicates is greatest when the read counts are the highest, and that OTUs with very small counts tend to have small differences. Plots such as this form the basis for using the Negative Binomial to estimate variability in RNA-seq datasets of this type, and are one of the motivations to use RNA-Seq based statistical procedures for 16S rRNA sequencing datasets\cite{McMurdie:2014a}.  

Note that the 99th percentile of the variability observed for all three sampling methods under-estimates the variability in the data when the counts are close to 0. Interestingly, the Jacknife and bootstrap estimation methods become non-linear in this region and severely under-estimate the variability seen for OTUs with less than 10 counts. In contrast the Dirichlet instances become slightly more variable when the feature counts are low. The reasons for the under-counting of technical variance by subsampling are obvious upon reflection: subsampling is constrained by the actual observations being sampled and so can only result in estimating \emph{fewer} reads per feature, never more.  

The right plot shows the same data plotted as the ratio between the counts observed for replicates A and B. When plotted in this way, it is clear that the rare OTUs, for which we have the least accurate estimate of their underlying abundance, show the largest  differences in ratio abundance. Conversely, the most abundant OTUs show the smallest amount of  variability. In this plot the range of variability estimated by Dirichlet sampling brackets the observed variability, and the subsampling approaches again under-estimate the variability of OTUs with very low numbers of counts. The Loess line of best fit appears to under-represent the variation when feature counts are high, but this is an artefact of the line fitting procedure.

 
\subsection{Scaling the data}
It is obvious that the data for each sample must be placed on a common scale, and while the logarithm of the counts reduces the difference between extreme values, the data is skewed because the minimum is the logarithm of the value assigned to 0 (in the examples above, this is $log2(0.5)=-1$) and the maximum value is proportional to count value of the most abundant OTU in the sample. So simply taking the logarithm while commonly used for proportional data, does not scale the data properly\cite{Aitchison:1986}. The solution devised by Aitchison, and validated and extended others\cite{pawlowsky2011compositional} is to centre the logarithmic transformation on the geometric mean as was done for the ratio transform used in Figure \ref{transform}. This is referred to aws the centred log-ratio transformation. This transformation has several effects. First, the data is on a common scale, where the values can be interpreted as the ratio between the count for each OTU and the geometric mean count for all OTUs in the sample. Second, the differences between values represent the ratios between values in that sample; i.e., a difference of 1 represents a two-fold abundance difference if the base of the logarithm is 2. Third, converting the count values to ratios preserves the 1:1 correspondence between the original values. Fourth, the relationships between values is not affected if particular OTUs are included or excluded from analysis. Finally, the ratio values are now linearly related and so can now be used in standard statistical analyses. Atichison\cite{Aitchison:1986}, Pawlsky-Glahn\cite{Pawlowsky-Glahn:2006}, and Egozcue\cite{egozcue2005}, have done much work to develop rigorous approaches to analyze such data types\cite{pawlowsky2011compositional}, and we have developed an R package that can be useful to determine differential abundances of OTUs in these datasets\cite{fernandes:2013,fernandes:2014}. 

\subsection{The power of compositional data analysis: more formal statement.}
A dataset is defined as compositional if it contains $D$ multiple parts, where each part is non-negative, and the sum of the parts is known (Aitchison 1986, pg 25). A composition containing $D$ parts where the sum is 1 can be formally stated as: $C_D = \{(x_1,x_2,x_3, \ldots x_D); x_1\ge 0, x_2\ge 0, x_3 \ge 0, \ldots x_D \ge 0; \sum_{x=1}^{D} = 1\}$. The sum of the parts is usually set to 1 or 100, but can take any value; i.e., any composition can be scaled to any arbitrary sum such as a ppm.  It is important to know that the values of the parts of compositional datasets are constrained because of the constant sum. The constant sum constraint causes the parts to have a negative correlation bias since an increase in the value of one part must be offset by a decrease in value of one or more other parts. Thus any correlation-based analysis is invalid in these datasets, as originally noted by Pearson\cite{Pearson:1896}. In addition, compositional datasets have the property that they are described by $D-1$ observations if the sum of the parts is known\cite{Aitchison:1986}. In other words, if we know that all parts sum to 1, then the last part can be known by subtracting the sum of all other parts from 1, i.e., $x_D = 1-\sum_{x=1}^{D-1}$. Graphically, this means that compositions inhabit a space called the Aitchison simplex that contains 1 fewer dimensions than the number of parts. The distances between parts on the Aitchison simplex are not linear, especially at the boundaries (see Figure \ref{constant}). This is important because all common statistical tests assume a that differences between parts are linear (or additive). Thus, while standard tests will produce output, the output will be misleading because distances on the simplex are non-linear and bounded. 

a <- runif(100, min=0, max=10)
b <- runif(100, min=0, max=10)
c <- runif(100, min=0, max=10)
abc <- rbind(a,b,c)
abc.comp <- apply(abc, 2, function(x){x/sum(x)})
abc.acomp <- acomp(abc)

par(mfrow=c(1,3))
scatterplot3d(t(abc))
scatterplot3d(t(abc.comp), xlim=c(0,1), ylim=c(0,1), zlim=c(0,1))
plot(t(abc.acomp))
%

\subsubsection{Sub-compositions:}~Compositional data also exhibit the unusual property that the examination of a sub-composition of these data will provide different answers than will be obtained with the full dataset\cite{Aitchison:1986}. This is problematic because 16S rRNA gene sequencing experimental designs are \emph{always} sub-compositions. Inspection of papers in the literature provide many examples. For example, it is common practice to discard rare OTU species prior to analysis and to re-normalize by dividing the counts for the remaining OTUs by the new sample sum. It is also common to use only one or a few taxonomic groupings to determine differences between experimental conditions. In the case of RNA-seq only the mRNA or miRNA is sequenced. All of these practices expose the investigator to the problem of non-coherence between sub-compositions.

\subsubsection{Spurious correlations:}~Finally, it is important to know that compositional data has the additional problem of  spurious correlation \cite{Pearson:1896}, and in fact this was the first troubling issue identified with compositional data. This phenomenon is best illustrated with  the following example from Lovell et. al\cite{Lovell:2015aa}, where they show how simply dividing two sets of random numbers (say abundances of OTU1 and OTU2), by a third set of random numbers (say abundances of OTU3) results in a strong correlation. Note that this phenomenon depends only on there being a common denominator.

Practically speaking this means that \emph{every microbial correlation network that has ever been published is suspect} unless it was determined using SPARCC \cite{Friedman:2012}, a tool that at least partially accounts for this spurious correlation. Lovell is in the process of producing an R package for the compositionally appropriate examination of correlations (personal communication).

\begin{figure}
\begin{center}

<<correlation,echo=FALSE,fig.width=5,fig.height=3.7>>=

n.obs <- 100
OTU.df <- data.frame(OTU1=rnorm(n.obs, mean=10, sd=1),
                      OTU2=rnorm(n.obs, mean=10, sd=1),
                      OTU3=rnorm(n.obs, mean=30, sd=4))
OTU.df <- transform(OTU.df,
                     OTU1.over.OTU3= OTU1/OTU3,
                     OTU2.over.OTU3= OTU2/OTU3)
plot(OTU.df$OTU1.over.OTU3, OTU.df$OTU2.over.OTU3, pch=19, cex=0.3,xlab="OTU1/OTU3", ylab="OTU2/OTU3")
#cor(OTU.df$OTU1.over.OTU3, OTU.df$OTU2.over.OTU3)
@
\caption{Spurious correlation in compositional data. Two random vectors drawn from a Normal distribution, were divided by a third vector also drawn at random from a Normal distribution. The two vectors have nothing in common, they should exhibit  no correlation, and yes they exhibit a correlation coefficent of $>0.65$ when divided by the third vector. See the introductory section of the Supplementary Information of Lovell\cite{Lovell:2015aa} for a more complete lay description of this phenomenon.  }
\label{correlation}
\end{center}
\end{figure}


Atichison\cite{Aitchison:1986}, Pawlsky-Glahn\cite{Pawlowsky-Glahn:2006}, and Egozcue\cite{egozcue2005}, have done much work to develop rigorous approaches to analyze compositional data\cite{pawlowsky2011compositional}. The essential step is to reduce the data to ratios between the $D$ parts as outlined above. This step moves the data from the Aitchison simplex and to the more familiar Euclidian space where the distances between parts are linear. However, the investigator must keep in mind that the distances are between ratios, not between counts. Several transformations are in common use, but the one most applicable to HTS data is the centred log-ratio transformation or clr, where the data in each sample is transformed by taking the logarithm of the the ratio between the count value for each part and the geometric mean count: i.e., for D features in sample X, $clr [x_1, x_2, x_3, \ldots x_D] = [log_2(x_1/gX), log_2(x_2/gX), log_2(x_3/gX) \ldots log_2(x_D/gX)]$ where $gX$ is the geometric mean of the features in sample X. This is the transformation described above.


\section{So how can I analyze compositional data?}

Fortunately, the analysis of compositional datasets has a well-developed methodology, much of which was worked out in the geological sciences. The following steps, and example code, is a step by step guide to examining a compositional 16S rRNA gene sequencing dataset in a more formally correct manner. This approach assumes that there is nothing really special about high-throughput sequencing data from the point of view of the analysis. The user should realize however that compositional data analysis is still an area of active research and the types of datasets typically found in high-throughput biology are particularly problematic because they are  high-dimensional datasets that contain many 0 values.

\subsection{An introduction to the compositional biplot}\  The compositional biplot is the essential workhorse tool for compositional analysis. Properly made and interpreted it summarizes all the essential results of your experiment. However, the weakness of this approach is that it is descriptive and exploratory, not quantitative. Quantitative tools can be applied later to support the conclusions derived from the biplot. 

We will illustrate this by examining a dataset from a clinical trial that examined the effect of treating women diagnosed as having bacterial vaginosis with either antibiotics, or antibiotics plus a probiotic supplement (Macklaim et.al, in press). For this example, I have extracted only the before and after treatment samples from the BV probiotic trial. Samples that are before treatment are identified as BXXX, where XXX is the sample identifier, and after treatment as AXXX. Samples are further identified as to their Nugent status, a rough indicator of whether the sample was from a women with BV or not: these are identified in the sample labels as `\_bv' or `\_n', some samples were indeterminate and are labeled as `\_i'. In addition, for this analysis,  individual OTUs have been aggregated to genus level using QIIME, except for \emph{L. iners} and \emph{L. crispatus} which remain as separate species in the tables.

We will use two datasets. First the set of all taxa, and second the set of taxa that are more abundant than 0.1\% in all samples. The following is a step-by-step guide with annotated code:

<<data,echo=TRUE,message=F>>=
# load the required R packages
require(compositions) # exploratory data analysis of compositional data
require(zCompositions) # used for 0 substitution
require(ALDEx2) # used for per-OTU comparisons
require(xtable) # used to generate tables from datasets

# load the data and the colours
d.pro.0 <- read.table("bbv_probiotic_samples.txt", header=T, row.names=1)

# remove awkward values from the names
rn <- gsub("_",".", rownames(d.pro.0))
rownames(d.pro.0) <- rn

# the first two rows and three columns of the data looks like this:
d.pro.0[1:2,1:3]

# a correspondence table of taxa and colours
col.tax <- read.table("bbv_colours.txt", header=T, row.names=1, comment.char="")

# again, change awkward characters in the row names
rownames(col.tax) <- gsub("_",".", rownames(col.tax))

# replace 0 values with the count zero multiplicative method and output counts
#
# this function expects the samples to be in rows and OTUs to be in columns 
# so the dataset is turned sideways on input, and then back again on output
# you need to know which orientation your data needs to be in for each tool

d.pro <- t(cmultRepl(t(d.pro.0), method="CZM", output="counts"))

# convert to proportions by sample (columns) using the apply function
d.pro.prop <- apply(d.pro, 2, function(x){x/sum(x)})

#####
# Make a dataset including every taxon

# order the taxa  in decreasing abundance 
d.names.all <- rownames(d.pro.prop)[ 
    order(apply(d.pro.prop, 1, sum), decreasing=T)]
d.pro.prop.abund.all <- d.pro.prop[d.names.all,]

# generate the compositional dataset as centre log-ratios
# acomp is from the compositions package
d.acomp <- acomp(t(d.pro.prop.abund.all))

#####
# Make a dataset where the taxon is more abundant than 0.1% in all samples

# remove all taxa that are less than 0.1\% abundant in any sample 
d.pro.abund.unordered <- d.pro[apply(d.pro.prop, 1, min) > 0.001,]

# add in the names again and sort by abundance
d.names <- rownames(d.pro.abund.unordered)[
    order(apply(d.pro.abund.unordered, 1, sum), decreasing=T) ]
    
# make a standard list of colours for plotting
colours <- as.character(col.tax[d.names,])

# get the taxa in the reduced dataset by name
d.pro.abund <- d.pro.abund.unordered[d.names,]

# make our compositional dataset
d.acomp.abund <- acomp(t(d.pro.abund))
@

The first key function here is the 0 replacement function  {\tt cmultRepl} which has many options. The bottom line is that the replacement of 0 values in these datasets is an area of ongoing research, and so there is no general way to treat 0 values in these datasets. The reader is encouraged to try different 0 replacement values and strategies and observe how it affects the conclusions. 

The second key function is the {\tt acomp} function. This makes an {\tt R S3} class dataset from the 0 replaced count dataset. This is essentially a dataset where the counts are treated as centre log-ratio values. 

\begin{figure}
\begin{center}

<<biplot_all,echo=TRUE,fig.width=8,fig.height=6>>=

# A biplot of the whole dataset

# generate a PCA object
pcx <- princomp(d.acomp)

layout(matrix(c(1,2),1,2, byrow=T), widths=c(6,2), height=c(6,4))

biplot(pcx, cex=0.5, col=c("black", "red"), arrow.len=0, scale=1)

# add a scree plot
plot(pcx, type="variance")

@
\caption{The left figure shows a covariance biplot of the complete dataset, the right figure shows a scree plot of the same data. The initial exploratory analysis of this dataset is not encouraging because  the amount of variance explained is fairly minimal with \Sexpr{round(sum(pcx$sdev[1]^2)/mvar(d.acomp), 3)} of the variance being explained by component 1, and \Sexpr{round(sum(pcx$sdev[2]^2)/mvar(d.acomp),3)} being explained by component 2. The scree plot shows that components 3 and 4 also explain substantial portions of the variance, hinting that some unknown factor is affecting our ability to extract information from the data. }
\label{biplot_all}
\end{center}
\end{figure}

Compositional biplots show both the amount of variance of both samples (shown in black) and variables (taxa, shown in red).  If substantial variation is explained by the first two principle components (which is not in this biplot!!), then the following rules can be used to examine the data:

\begin{enumerate}

\item the rays in this plot show the amount of variance exhibited by each taxon relative to the centre of the dataset where longer rays mean more variation across all samples.

\item the location of the sample name shows how variable it is relative to other samples.

\item samples that are highly variable, and that are in the same direction as a long ray for a taxon will contain that taxon in high abundance For example, Firmicutes:Streptococcus will be abundant in sample B260\_bv: it composes 37\% of the total. The inverse is also true.

\item taxa where the tips of the rays are co-incident and of the same length indicate that the ratio between those two taxa are nearly identical across all samples For example: Actinobacteria:Corynebacterium and Firmicutes:Lactobacillus.iners in the lower right corner.

\item taxa where the angles between the rays are orthogonal are uncorrelated.

\item taxa where the tips of the rays are very distant from each other, regardless of whether the link between the tips passes through the origin, indicated highly variable ratios across the samples for example: Actinobacteria:Bifidobacterium and Firmicutes:Lactobacillus.iners in the lower left and lower right corners.

\item three or more taxa lying on a common link will be positively or negatively correlated. For example, the link between TM7.genera.incertae.sedis and Firmicutes:Streptococcus passes directly through Actinobacteria:unclassified. These will show strong positive or negative correlations

\item the angle between links contains information about correlations between pairs, or groups of ratios. For example, the link between Actinobacteria:Bifidobacterium and Firmicutes:Lactobacillus.iners, and the link between Firmicutes:Streptococcus and Actinobacteri:Mobiluncus intersect at an angle of approximately 90 degrees. Thus, these two ratios are uncorrelated 

\end{enumerate} 

However, the biplot and the scree plot in figure \ref{biplot_all} are not encouraging.  The longest ray is for the taxon identified as unclassified:TM7.genera.incertae.sedis, and the next longest rays are for two different members of the Actinobacteria phylum. An examination of the abundances in Table \ref{abundances} given in the Appendix suggests that many of the taxa that exhibit the most variance have the lowest read counts. If you recall from Figure \ref{rarefy}, the reproducibility of taxa with low read counts is very poor. Thus, it is likely that many if not most of these variances are spurious in this type of dataset. In fact, we have to get to read counts of approximately 10 before the ratio between replicates is less than 2. This highlights one important reason to remember that the data are ratio data: the error structure is not what you think because most of your technical variation is in the taxa with the lowest number of counts. Thus, we could try filtering to remove taxa that are  rare in all samples to see if this improves the resolution of our data. 

Lets see how filtering to remove rare taxa changes our explanatory ability. We previously made a reduced dataset filtered by including only taxa that are more abundant than 0.1\% in all samples, and we now examine this reduced dataset. 

\begin{figure}
\begin{center}

<<biplot_abund,echo=TRUE,fig.width=8,fig.height=6>>=

pcx.abund <- princomp(d.acomp.abund)
layout(matrix(c(1,2),1,2, byrow=T), widths=c(6,2), height=c(6,4))
biplot(pcx.abund, cex=0.7, col=c("black", "red"), arrow.len=0, scale=1, expand=0.8)

plot(pcx.abund, type="variance")

@
\caption{The left figure shows a covariance biplot of the abundance-filtered dataset, the right figure shows a scree plot of the same data. This exploratory analysis is much more encouraging because  the amount of variance explained is now rather substantial with \Sexpr{round(sum(pcx.abund$sdev[1]^2)/mvar(d.acomp.abund), 3)} of the variance being explained by component 1, and \Sexpr{round(sum(pcx.abund$sdev[2]^2)/mvar(d.acomp.abund),3)} being explained by component 2. The scree plot also shows that the majority of the variability is on component 1. We can now interpret this biplot with some confidence. }
\label{biplot_abund}
\end{center}
\end{figure}

The biplot made from the abundance filtered dataset in Figure \ref{biplog_abund} is much more informative. The first two components now explain  \Sexpr{round (sum(pcx.abund$sdev[1]^2)/mvar(d.acomp.abund) + sum(pcx.abund$sdev[2]^2)/mvar(d.acomp.abund) ,2 ) } of the variance in the data, and we can observe some structure both in the taxa and in the samples. The left side of the plot contains all of the organisms commonly observed in BV, and the right side of the plot contains only members of the genus \emph{Lactobacillus}; indicating a clear split in the makeup of the samples. When we focus on the location of the samples,  the majority of the before treatment samples are on the left hand side of the plot, and the majority of the after treatment samples are on the right hand side. 

Applying the 8 rules of interpretation given above (and using only the genus name for brevity), we can see that:

\begin{enumerate}
\item \emph{L. iners} has the longest ray, and among these taxa is the most variable organism across samples, \emph{Gardnerella} has the shortest ray and so is the least variable.

\item The sample A238\_i is the sample that is least similar to any other sample because it is furthest from the centre (top left corner). It likely contains a substantial fraction of \emph{Sneathia} and little \emph{Megasphaera} and \emph{BVAB2}.

\item The sample A238\_i will contain a substantial proportion of \emph{Sneathia} and a very small amount of \emph{Lactobacillus} because the rays for these taxa are parallel to a ray that would connect this sample to the origin. The converse will be true for sample A314\_n. 

\item The two closest tips are for \emph{Megasphaera} and \emph{BVAB2}, thus the ratio between these two taxa is relatively constant across samples. Thus, each will be abundant when the other is abundant and \emph{vice versa}.

\item The abundance of \emph{Sneathia} and the taxa \emph{Gardnerella}, \emph{Megasphaera}, \emph{BVAB2} will be uncorrelated because these rays are approximately orthogonal.

\item The link between \emph{L. iners} and \emph{Sneathia} (and many others is very long), indicating that the ratios between these taxa are extremely variable. That is, when \emph{L. iners} is abundant, then others will be correspondingly rare.

\item The link between \emph{Prevotella} and \emph{L. crispatus} passes directly through \emph{Atopobium}. This indicates that these three taxa are linearly related. In this case, it is clear when \emph{L. crispatus} increases, the other two will decrease. 

\item The link between \emph{BVAB2} and \emph{Sneathia} and the link in the previous item intersect at approximately 90 degrees. Thus the ratios of the last two taxa will be uncorrelated with the ratios of the previous three taxa.

\end{enumerate}

This biplot suggests some structure in the BV samples that is related to the abundance of \emph{Sneathia}. The evidence for this is that the abundance of this genus is not correlated with the abundance of the others that are commonly found in BV. This `pulls' several samples towards the top right corner. Investigation of other datasets would be required to test this observation.

\subsubsection{Cluster analysis}

\begin{figure}
\begin{center}

<<cluster,echo=TRUE,fig.width=7,fig.height=4>>=
# generate the distance matrix
dd <- dist(d.acomp.abund, method="euclidian")

# cluster the data
hc <- hclust(dd, method="ward.D2")

# now re-order the data to plot the barplot in the same order
d.order <- d.pro.abund[,hc$order]
d.order.acomp <- acomp(t(d.order))

layout(matrix(c(1,3,2,3),2,2, byrow=T), widths=c(6,2), height=c(4,4))
par(mar=c(2,1,1,1)+0.1)

plot(hc, cex=0.6)
barplot(d.order.acomp, legend.text=F, col=colours, axisnames=F, border=NA, xpd=T)
par(mar=c(0,1,1,1)+0.1)
plot(1,2, pch = 1, lty = 1, ylim=c(-20,20), type = "n", axes = FALSE, ann = FALSE)
legend(x="center", legend=d.names, col=colours, lwd=5, cex=.6, border=NULL)
@
\caption{Unsupervised clustering of the reduced dataset. The top figure shows a dendrogram of relatedness generated by unsupervised clustering of the Aitchison distances. The bottom figure shows a stacked bar plot of the samples in the same order. The legend indicating the colour scheme for the taxa is on the right side. }
\label{cluster}
\end{center}
\end{figure}

The result of the biplot suggested that there were two groups that could be defined with this set of data. With a few exceptions, there appears to be a fairly strong separation between  the samples containing a majority of \emph{Lactobacillus} sp., and those lacking them. We can explore this by performing a cluster analysis. In the traditional microbiome analysis methods, clustering is based on the weighted or unweighted unifrac distances or on the Bray-Curtis dissimilarity metric. These metrics are much more sensitive to the makeup of the community than is the Aitchison distance used in compositional data analysis. Thus, here we will use the Aitchison distance metric which fulfills the criteria required for compositional data. In particular, by using a compositional approach, it is appropriate to examine a defined sub-composition of the data. 


The results of unsupervised clustering of the dataset is shown in Figure \ref{cluster}. Here we can use Euclidian distance because the Aitchison transformed data are linearly related and in placed in the familiar space. However, the user must remember that all distances are calculated from the ratios between taxa, and not on the taxa abundances themselves! For this figure we are using the ward.D2 method which clusters groups together by their squared distance from the geometric mean distance of the group. There are many other options, and the user should choose one that best represents the data. 

The cluster analysis shows the split between two types of samples rather clearly. Samples containing an abundance of \emph{Lactobacillus} sp. are grouped together on the right, and samples with an abundance of other taxa are grouped together on the left. 

The results of the cluster analysis can help explain and clarify the compositional biplot. For example, the four samples in the middle lower part of the biplot in Figure \ref{biplot_abund} labelled A/B312 and A/B282, group together in both the biplot and the cluster plot. These samples are atypical for both the N and BV groups, The cluster plot and associated marplot show that they contain substantially more of the \emph{Lactobacillus} taxon, and somewhat more of the taxa normally found in BV than in the other N samples. Based on these two results it would be appropriate to exclude these four samples from further analysis because of their atypical makeup. 

As indicated from the biplot, the abundance of \emph{Gardnerella} sp. is not a good  discriminator between the  two groups because it may be abundant or rare in either group. This observation ... (\textbf{Jean help please}).

\subsection{Univariate differences between groups}

We will now conduct a univariate comparison between the B and A groups, for simplicity, we will keep the four outlier samples, but the reader is encouraged to remove them and see how the results change. For this, we will use the ALDEx2 tool, that incorporates a Bayesian estimate of taxon abundance into a compositional framework. Here is the code:

<<aldex.data,echo=TRUE>>=
# generate the dataset 
d.B <- colnames(d.pro.0)[grep("B", colnames(d.pro.0))]
d.A <- colnames(d.pro.0)[grep("A", colnames(d.pro.0))]
d.aldex <- data.frame(d.pro.0[,d.B], d.pro.0[,d.A])

# make the list of set membership
conds <- c(rep("Be", 31), rep("Af", 31))

x <- aldex.clr(d.aldex, mc.samples=256, verbose=FALSE)
x.t <- aldex.ttest(x, conds)
x.e <- aldex.effect(x, conds, verbose=FALSE)
x.all <- data.frame(x.e,x.t)
@

The ALDEx2 tool estimates the distribution of taxon abundance by sampling from a Dirichlet distribution with the results outlined in Figure \ref{rarefy}. This takes the original input data, and generates a distribution of posterior probabilities of observing each taxon. This distribution is transformed by the centred log-ratio transformation, and is used to conduct univariate statistical tests. These tests return a distribution of P and Benjamini-Hochberg adjusted P values, and the tool reports the mean of these distributions. In this way, we account for the large variation in these datasets, and identify only those taxa whose difference between the groups is robust to sampling variation.   

We need to supply the table of counts and a list that outlines which group each sample belongs to. Following that, we generate the distribution of posterior probabilities using the {\tt aldex.clr} function, then conduct the statistical tests and determine effect sizes using the {\tt aldex.ttest} and {\tt aldex.effect}. Finally, we can plot the results using {\tt aldex.plot}. 

The output table contained in {\tt x.all} contains much information regarding your dataset, and is used to generate the output plot: see the documentation for ALDEx2 for a complete description of each entry in the table. The most important data for the purposes of comparison are those given in Table \ref{sig.table}. All information in this table, except P value information, is on a log2 scale. Here we have the difference between groups, the maximum difference within groups (variance), the effect size calculated as $\frac{diff.btw}{diff.win}$, the overlap between the Bayesian distributions of group A and B, and finally the raw expected P value, and the expected Benjamini-Hochberg (BH) adjusted value.  

When interpreting these results you should remember that you are actually examining ratios between values, rather than abundances. So abundances are determined as the ratio of the abundance of a taxon to all taxa in the sample. The user should also remember that all values reported are the mean values over the number of Dirichlet instances as given by the {\tt mc.samples} variable in the {\tt aldex.clr} function.
 
 <<sig.table, echo=TRUE,results="asis">>=
sig <- which(x.all$wi.eBH <= 0.05)
# make the table
xtable(
   x.all[sig,c(4:7, 10,11)], caption="Table of significant taxa", digits=3,
        label="sig.table", align=c("l",rep("r",6) ) 
)
@

In the examples given in Table \ref{sig.table}, we filtered to print only those taxa where the expected BH values was less than 0.05, meaning that the expected likelihood of a false positive identification \emph{per taxon} is less than that threshold. Using \emph{L. iners} as an example, we can see that the absolute difference between groups can be up to \Sexpr{round(x.all["Firmicutes:Lactobacillus.iners",4], digits=2)}, implying that the absolute fold change in the ratio between \emph{L. iners} and all other taxa between groups for this organism is on average \Sexpr{round(2^abs(x.all["Firmicutes:Lactobacillus.iners",4]), digits=2)} fold across samples. However, note that the difference within is even larger, giving an effect size of \Sexpr{round(x.all["Firmicutes:Lactobacillus.iners",6], digits=2)}. Thus, we can see that the difference between groups is less than the variability within a group, a result that is typical for microbiome studies. 

We can examine these data graphically as shown in Figure \ref{aldex}. The left panel of this figure shows a plot of the within to between condition differences, with the red dots representing those that have a BH adjusted P value of 0.05 or less. Taxa that that are more abundant than the mean in the BV samples have positive y values, and those that are more abundant than the mean in the N samples have negative y values. We refer to these as `effect size' plots, and they summarize the data in an intuitive way. The grey lines represent the line of equivalence for the within and between group values. Black dots are taxa that are less abundant than the mean taxon abundance: here it is clear that the abundance of these taxa, in general, are difficult to estimate with any precision.

The middle plot in Figure \ref{aldex} shows a plot of the effect size vs. the BH adjusted P value, and we can see the strong correspondence between these two measures. In general, we prefer to use an effect size cutoff because this is more robust than are P values. The right plot in this figure shows a volcano plot for reference.

\begin{figure}
\begin{center}
<<aldex,echo=TRUE,fig.width=6,fig.height=4>>=

layout(matrix(c(1,2,3,1,2,3),2,3, byrow=T), widths=c(4,2,2), height=c(4,4))
par(mar=c(5,4,4,1)+0.1)
aldex.plot(x.all, test="wilcox", cutoff=0.05, all.cex=0.8, called.cex=0.8)
plot(x.all$effect, x.all$wi.eBH, log="y", pch=19, main="Effect",
    cex=0.5, xlab="Effect size", ylab="Expected Benjamini-Hochberg P")
plot(x.all$diff.btw, x.all$wi.eBH, log="y", pch=19, main="Volcano",
    cex=0.5, xlab="Difference", ylab="Expected Benjamini-Hochberg P")
@
\caption{Examination of univariate differences between groups. The left plot shows a plot of the maximum variance within the B or A group vs. the difference between groups. Red points indicate those that have a mean Benjamini-Hochberg adjusted P-value of 0.05 or less using P values calculated with the Wilcoxon rank test. The middle plot shows a plot of the effect size vs. the adjusted P value. In general, effect size measures are more robust than are P values and are preferred. For a large sample size such as this one, an effect size of 0.5 or greater will likely correspond to biological relevance. The right plot shows a volcano plot where the difference between groups is plotted vs the adjusted P value.}
\label{aldex}
\end{center}
\end{figure}

\section{Conclusions}
We have shown that 16S rRNA gene sequencing datasets, and others of the same type including RNA-Seq datasets, are logically best treated as ratios because the total number of reads is uninformative, and the resulting values are best interpreted as fold-changes. We showed that treating the data as ratios where the denominator is the geometric mean for a sample accurately recapitulates the shape and the error profile of the input data. We used with Dirichlet Monte-Carlo replicates coupled with the centred log-ratio transformation to show that point-estimates of statistical significance in a real dataset can substantially inflate the observed P value because of random partitioning of low count values across datasets. Finally, we provide a worked example of how to examine a published 16S rRNA gene sequencing dataset and have given guidance in how to interpret the results in both a multivariate and univariate way.

In essence, we argue that 16S rRNA gene sequencing datasets, RNA-seq datasets, and many other -seq datasets are not special and do not each need their own unique statistical analysis approach. The data generated can be examined by a general multivariate approach after accounting for the compositional nature of the data, and such an analysis is comparable or superior to the domain-specific approaches\cite{fernandes:2014, fernandes:2014, macklaim:2013, mcmurrough:2014, Goneau:2015aa, Lovell:2015aa} . Interested readers should consult the compositional data literature, but in particular three books are useful: the original by Aitchison in 1986 \cite{Aitchison:1986}, a book outlining how to use the compositions R package\cite{van2008compositions}, and finally a comprehensive book that outlines the essential geometric problem of compositional data as it is understood at present\cite{pawlowsky2015modeling}. 


\bibliography{bibdesk_refs}

\section{Supplement}
 
\subsubsection{Table of proportions}
<<kable, echo=TRUE,results="asis">>=
# generate a summary of the proportional data, remember that we have performed a 0 replacment
s.x <- summary(t(as.data.frame(d.pro.prop.abund.all)))
# remove nuisance labels
s.x <- gsub("1st Qu.:", "", s.x)
s.x <- gsub("Median :", "", s.x)
s.x <- gsub("3rd Qu.:", "", s.x)
d.tab <- data.frame( as.numeric(s.x[2,]), as.numeric(s.x[3,]), as.numeric(s.x[5,]) )
colnames(d.tab) <- c( "1st Q", "median", "3rd Q")
rownames(d.tab) <- colnames(s.x)

# make the table
xtable(
   d.tab, caption="Summary of abundances", digits=5, label="abundances", align=c("l","r", "r", "r")
)
@



\end{document}  